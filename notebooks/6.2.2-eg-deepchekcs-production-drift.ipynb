{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from skimage import io, filters\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "from deepchecks.vision.vision_data import BatchOutputFormat\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.utils.data import DataLoader\n",
    "from deepchecks.vision import VisionData\n",
    "from deepchecks.vision.suites import model_evaluation, train_test_validation, data_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED = [\n",
    "    '../data/processed/datasets_processed/training_data/',\n",
    "    '../data/processed/datasets_processed/training_labels/',\n",
    "    '../data/processed/datasets_processed/testing_data/',\n",
    "    '../data/processed/datasets_processed/testing_labels/'\n",
    "]\n",
    "RATIO = 1\n",
    "MODEL_PATH = '/models/saved/10_50_0.1_0.25_unet_model.pth'\n",
    "BEST_MODEL = 'https://cdn.albertovalerio.com/datasets/crop_segmentation/models/my_unet_model.pth'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "LABEL_MAP = {0: 'no-weed', 1: 'weed'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = '.DS_Store'\n",
    "train_i = os.listdir(PROCESSED[0])\n",
    "train_m = os.listdir(PROCESSED[1])\n",
    "test_i = os.listdir(PROCESSED[2])\n",
    "test_m = os.listdir(PROCESSED[3])\n",
    "if ko in train_i: train_i.remove(ko)\n",
    "if ko in train_m: train_m.remove(ko)\n",
    "if ko in test_i: test_i.remove(ko)\n",
    "if ko in test_m: test_m.remove(ko)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "\ttest_i,\n",
    "\ttest_m,\n",
    "\ttrain_size=.7,\n",
    "\ttest_size=.3,\n",
    "\trandom_state=3,\n",
    "\tshuffle=True\n",
    ")\n",
    "\n",
    "sampleT = int(len(X_train) * RATIO)\n",
    "sampleV = int(len(X_val) * RATIO)\n",
    "\n",
    "train_d = [[io.imread(PROCESSED[2]+v), io.imread(PROCESSED[3]+y_train[:sampleT][i])] for i, v in enumerate(X_train[:sampleT]) if io.imread(PROCESSED[2]+v).any()]\n",
    "prod_d = [[io.imread(PROCESSED[2]+v), io.imread(PROCESSED[3]+y_val[:sampleV][i])] for i, v in enumerate(X_val[:sampleV]) if io.imread(PROCESSED[2]+v).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_data(dataset, low, high):\n",
    "\n",
    "    random.seed()\n",
    "    for img in dataset:\n",
    "        img[0] = filters.gaussian(img[0], sigma = 2, channel_axis=-1)\n",
    "        x = random.randint(0,100)\n",
    "        if x%2==0:\n",
    "            img[0] = img[0]*low\n",
    "        else:\n",
    "            img[0] = img[0]*high\n",
    "        img[0] *= 255\n",
    "        img[0] = img[0].astype(np.uint8)\n",
    "      \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_d = production_data(prod_d, 0.5, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\ttransforms.ToPILImage(),\n",
    "\ttransforms.PILToTensor(),\n",
    "\ttransforms.ConvertImageDtype(torch.float)\n",
    "])\n",
    "train_dataset = [[transform(i[0]), transform(i[1])[0]] for i in train_d]\n",
    "production_dataset = [[transform(i[0]), transform(i[1])[0]] for i in production_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 71\n",
      "Number of test images: 31\n",
      "Example output of an image shape: torch.Size([3, 360, 480])\n",
      "Example output of a label shape: torch.Size([360, 480])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training images: {len(train_dataset)}')\n",
    "print(f'Number of test images: {len(production_dataset)}')\n",
    "print(f'Example output of an image shape: {train_dataset[0][0].shape}')\n",
    "print(f'Example output of a label shape: {train_dataset[0][1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv1 = nn.Conv2d(inChannels, outChannels, 3)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.conv2 = nn.Conv2d(outChannels, outChannels, 3)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.conv2(self.relu(self.conv1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self, channels=(3, 16, 32, 64)):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encBlocks = nn.ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1]) for i in range(len(channels) - 1)]\n",
    "\t\t)\n",
    "\t\tself.pool = nn.MaxPool2d(2)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tblockOutputs = []\n",
    "\t\tfor block in self.encBlocks:\n",
    "\t\t\tx = block(x)\n",
    "\t\t\tblockOutputs.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\t\treturn blockOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, channels=(64, 32, 16)):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.channels = channels\n",
    "\t\tself.upconvs = nn.ModuleList(\n",
    "\t\t\t[nn.ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "\t\t\t\tfor i in range(len(channels) - 1)]\n",
    "\t\t)\n",
    "\t\tself.dec_blocks = nn.ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1]) for i in range(len(channels) - 1)]\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x, encFeatures):\n",
    "\t\tfor i in range(len(self.channels) - 1):\n",
    "\t\t\tx = self.upconvs[i](x)\n",
    "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
    "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
    "\t\t\tx = self.dec_blocks[i](x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef crop(self, encFeatures, x):\n",
    "\t\t(_, _, H, W) = x.shape\n",
    "\t\tencFeatures = CenterCrop([H, W])(encFeatures)\n",
    "\t\treturn encFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\tself, outSize, encChannels=(3, 16, 32, 64),\n",
    "\t\tdecChannels=(64, 32, 16), retainDim=True\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = Encoder(encChannels)\n",
    "\t\tself.decoder = Decoder(decChannels)\n",
    "\t\tself.head = nn.Conv2d(decChannels[-1], 1, 1)\n",
    "\t\tself.retainDim = retainDim\n",
    "\t\tself.outSize = outSize\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tencFeatures = self.encoder(x)\n",
    "\t\tdecFeatures = self.decoder(encFeatures[::-1][0], encFeatures[::-1][1:])\n",
    "\t\tmyMap = self.head(decFeatures)\n",
    "\t\tif self.retainDim:\n",
    "\t\t\tmyMap = F.interpolate(myMap, self.outSize)\n",
    "\t\treturn myMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('my_unet_model.pth'):\n",
    "    os.system('wget %s'%BEST_MODEL)\n",
    "model = torch.load('my_unet_model.pth', map_location=torch.device(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, input):\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToPILImage(),\n",
    "\t\ttransforms.PILToTensor(),\n",
    "\t\ttransforms.ConvertImageDtype(torch.float)\n",
    "\t])\n",
    "\tif isinstance(input, str):\n",
    "\t\timage = io.imread(input)\n",
    "\telse:\n",
    "\t\timage = input\n",
    "\timage = transform(image)\n",
    "\timage = image.unsqueeze(0)\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tpredMask = model(image.to(DEVICE)).squeeze()\n",
    "\t\tpredMask = torch.sigmoid(predMask)\n",
    "\t\tpredMask = predMask.cpu().numpy()\n",
    "\t\tpredMask = (predMask > .5) * 255\n",
    "\t\tpredMask = predMask.astype(np.uint8)\n",
    "\treturn predMask\n",
    "\n",
    "def convert_prediction(mask):\n",
    "\tx = mask.shape[0]\n",
    "\ty = mask.shape[1]\n",
    "\tmask_1 = np.zeros((x, y), dtype='float64')\n",
    "\tmask_2 = np.zeros((x, y), dtype='float64')\n",
    "\tfor i in range(x):\n",
    "\t\tfor j in range(y):\n",
    "\t\t\tif mask[i][j] == 0:\n",
    "\t\t\t\tmask_1[i][j] = 1.\n",
    "\t\t\tif mask[i][j] == 255:\n",
    "\t\t\t\tmask_2[i][j] = 1.\n",
    "\treturn nn.functional.softmax(torch.tensor(np.array([mask_1, mask_2])), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepchecks_collate_fn(batch) -> BatchOutputFormat:\n",
    "\n",
    "    # batch received as iterable of tuples of (image, label) and transformed to tuple of iterables of images and labels:\n",
    "    batch = tuple(zip(*batch))\n",
    "\n",
    "    # images:\n",
    "    images = [(tensor.numpy().transpose((1, 2, 0)) * 255).astype(np.uint8) for tensor in batch[0]]\n",
    "\n",
    "    #labels:\n",
    "    labels = [mask.type(torch.int8) for mask in batch[1]]\n",
    "\n",
    "    #predictions:\n",
    "    predictions = [make_predictions(model, img) for img in images]\n",
    "    predictions = [convert_prediction(pred) for pred in predictions]\n",
    "\n",
    "    return BatchOutputFormat(images=images, labels=labels, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, collate_fn=deepchecks_collate_fn)\n",
    "production_loader = DataLoader(dataset=production_dataset, shuffle=True, collate_fn=deepchecks_collate_fn)\n",
    "\n",
    "training_data = VisionData(batch_loader=train_loader, task_type='semantic_segmentation', label_map=LABEL_MAP)\n",
    "production_data = VisionData(batch_loader=production_loader, task_type='semantic_segmentation', label_map=LABEL_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177c2716dc514544928394445ea23c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<div style=\"display:flex; flex-direction: column; gap: 10px;\">\\n                <di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "production_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.vision.checks import PredictionDrift, ImageDatasetDrift\n",
    "\n",
    "check = PredictionDrift()\n",
    "result = check.run(train_dataset=training_data, test_dataset=production_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc1a354a2414111ad8a9526d00827f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Prediction Drift</b></h4>'), HTML(value='<p>    Calculate prediction drift b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../reports/linters/6.2.3-report-deepchecks-production-prediction-drift.html'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.show()\n",
    "result.save_as_html('../reports/linters/6.2.3-report-deepchecks-production-prediction-drift.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check = ImageDatasetDrift()\n",
    "result = check.run(train_dataset=training_data, test_dataset=production_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58c8f609d0d48ff9680d355fc858e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Image Dataset Drift</b></h4>'), HTML(value='<p>Calculate drift between the e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../reports/linters/6.2.4-report-deepchecks-production-dataset-drift.html'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.show()\n",
    "result.save_as_html('../reports/linters/6.2.4-report-deepchecks-production-dataset-drift.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
